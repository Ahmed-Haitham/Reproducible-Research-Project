---
title: "Taxi Price prediction in New York with Machine Learning"
author: 
  - name: Zimovska, Irena
    url: 
    affiliation: University of Warsaw, Faculty of Economic Sciences 
date: "March, 2024"
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
editor_options: 
  markdown: 
    wrap: 72
---

# Introduction 

In the domain of transportation and urban analytics, predictive modeling can play a useful role in forecasting taxi fares, a critical aspect of urban transportation and customer  decision-making. This project delves into the application of Machine Learning algorithms for predicting taxi fares in New York based on geospatial point data.

Taxi fare prediction entails estimating the fare amount for a taxi trip based on various factors such as pickup and drop-off locations, trip distance, time of day, and other relevant features. Given the complex interplay of spatial and temporal dynamics inherent in taxi operations, machine learning algorithms offer a powerful approach to model and predict fare amounts accurately.

This case study applies modelling with: 
* Random Forest Bagging 
* eXtreme Gradient Boosting (XGBoost) 
* Adaptive Boosting 

You may also find interesting the part of Feature Engineering and workarounds in the Data Cleaning part. But no spoilers here! :)

```{r}
# settings
options(scipen = 999)
Sys.setLanguage('english')
```

```{r message=FALSE, warning=FALSE}
# libraries import 
if (!require('pacman')) install.packages('pacman')
pacman::p_load(here, DescTools,dplyr, tidyverse, ggplot2, spdep, #rgdal,
               maptools, sp, RColorBrewer, e1071, spatstat, 
               dbscan, sf, geosphere, lubridate, dbscan, tidymodels, 
               finetune, mapview, ggmap, tigris, rgdal, httr, caret, gbm, xgboost, 
               kmeans, fvizclust, flexclust, factoextra, ClusterR, tree, randomForest,
               recipes, rsample, workflows, tune, parsnip, dials, finetune, distill)
```

# Data Exploration

```{r}
# import data
data <- read.csv(here("data/taxi_data.csv"))
```

The dataset contains information about **90 000** taxi trips and includes the following fields:

* id: An identifier for each taxi trip record. [it will be omitted]
* dropoff_latitude: The latitude coordinate of the dropoff location.
* dropoff_longitude: The longitude coordinate of the dropoff location.
* fare_amount: The fare amount for the taxi trip, representing the cost of the ride. [the outcome]
* feat01-feat10: Engineered features, potentially bringing more information.
* passenger_count: The number of passengers in the taxi during the trip.
* pickup_datetime: The date and time when the taxi trip started (in UTC).
* pickup_latitude: The latitude coordinate of the pickup location.
* pickup_longitude: The longitude coordinate of the pickup location.
* key: A unique key representing each taxi trip record. [same as pickup_datetime, so will be omitted as well]

```{r}
glimpse(data)
```


```{r}
# omit two columns 
data <- data %>% dplyr::select(-c('id', 'key'))
```


```{r}
data$passenger_big_group <- as.factor(ifelse(data$passenger_count <= 4, 0, 1))
plot(data$passenger_big_group)
```

```{r}
# DescTools package has a very nice function <Desc> which allows for an easy inspect of data with one line of code
DescTools:: CompleteColumns(data, which = FALSE)
Desc(data)
```

Surprisingly, we have a dataset with no missings! 


```{r}
data %>%
  ggplot(aes(x = fare_amount)) + 
  geom_histogram(aes(y = after_stat(density)), 
                 colour = "black", 
                 fill = "pink") +
  geom_density(alpha = .4, fill = "darkblue", bw = 3) +
  theme_bw()
```
```{r}
# Create a Q-Q plot of fare_amount
qqnorm(data$fare_amount)
qqline(data$fare_amount)
```

```{r}
summary(data$fare_amount)
```

```{r}
data <- data %>% mutate(fare_amount_log = log(fare_amount))
```

```{r}
data %>%
  ggplot(aes(x = fare_amount_log)) + 
  geom_histogram(aes(y = after_stat(density)), 
                 colour = "black", 
                 fill = "pink") +
  geom_density(alpha = .4, fill = "darkblue", bw = 0.1) +
  theme_bw()
```

```{r}
# Create a Q-Q plot of fare_amount
qqnorm(data$fare_amount_log)
qqline(data$fare_amount_log)
shapiro.test(sample(data$fare_amount_log, 5000))

```

```{r eval=FALSE, cache=TRUE, include=FALSE}
ggmap::register_google(key = "key", write = TRUE)
```


```{r, cache=TRUE}
ggmap::geocode("New York")
get_googlemap(center = "New York") %>% ggmap()
```


There is an issue of zero coordinates in data, which simply cannot work like that. 
We impute those with some random coordinates within a specified border box. 

# Data Cleaning


```{r}
data.sf <- st_as_sf(data, coords = c("pickup_longitude", "pickup_latitude"), crs = 4326)
```



```{r}
nyc <- st_read(here('data/nyc-boundaries/geo_export_9ca5396d-336c-47af-9742-ab30cd995e41.shp'))
```

```{r, cache=TRUE}
mapview(nyc, zcol=NULL, alpha.regions=0.5)
```


### Pickup missings 

```{r}
# Filter the data 
missing_pickup_coords <- (data$pickup_longitude == 0 | data$pickup_longitude > -72) | data$pickup_latitude == 0

# Generate random coordinates for imputation in the borders of NYC sf object
random_coordinates <- st_sample(nyc, sum(missing_pickup_coords)) %>% st_coordinates()

# Impute the missing pickup coordinates with random coordinates
data$pickup_longitude[missing_pickup_coords] <- random_coordinates[, "X"]
data$pickup_latitude[missing_pickup_coords] <- random_coordinates[, "Y"]

# Print the modified dataframe
print(data)
```

### Drop-off missings 

```{r}
missing_dropoff_coords <- (data$dropoff_longitude == 0|data$dropoff_longitude < -75) | (data$dropoff_latitude == 0| data$dropoff_latitude > 42)

# Generate random coordinates for imputation in the borders of NYC sf object
random_coordinates <- st_sample(nyc, sum(missing_dropoff_coords)) %>% st_coordinates()

# Impute the missing pickup coordinates with random coordinates
data$dropoff_longitude[missing_dropoff_coords] <- random_coordinates[, "X"]
data$dropoff_latitude[missing_dropoff_coords] <- random_coordinates[, "Y"]

```

 
# Feature Engineering 

We will create a couple of new columns: 
* trip distance - a Haversine distance calculated with the coordinates of pickup and drop-off
* year 
* month 
* day 
* hour 
* location clusters


### Trip distance 

```{r}

data <- data %>%
  mutate(
    trip_distance = apply(select(., pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude), 1, function(row) {
      distm(row[c("pickup_longitude", "pickup_latitude")], row[c("dropoff_longitude", "dropoff_latitude")])
    })
  )

```


```{r}
summary(data$trip_distance)
Desc(data$trip_distance)
```

### Date aggregates 

```{r}
# Assuming your dataframe is named 'data'
data$pickup_datetime <- ymd_hms(data$pickup_datetime)

data$year <- year(data$pickup_datetime)
data$month <- month(data$pickup_datetime)
data$day <- day(data$pickup_datetime)
data$hour <- hour(data$pickup_datetime)
```


# Train-Test Split 


```{r}
set.seed(555)
train_test_split <- data %>% initial_split(strata = fare_amount)

data.train <- training(train_test_split)
data.test  <- testing(train_test_split)
```



### Pick-Up clusters 

K-Means clustering with `kcca` package, which allows to predict the clusters for test data. 

```{r}
pickup_clusters <- kcca(data.train[, c('pickup_longitude', 'pickup_latitude')], k=6, kccaFamily('kmeans'))
```

```{r}
data.train$pickup_cluster <- as.factor(predict(pickup_clusters))
data.test$pickup_cluster <- as.factor(predict(pickup_clusters, newdata=data.test[, c('pickup_longitude', 'pickup_latitude')]))
```

```{r}
#mapview(data.test, xcol = "pickup_longitude", ycol = "pickup_latitude", crs = 4269, grid = FALSE, zcol = 'pickup_cluster')
```

```{r}
data$pickup_cluster <- NA 
```

```{r}
data[train_test_split$in_id, "pickup_cluster"] <- data.train$pickup_cluster
data[is.na(data$pickup_cluster), "pickup_cluster"] <- data.test$pickup_cluster
```

```{r}
train_test_split$data <- data
```

# Select features & build a model 

```{r}
# Define model formula 
model.formula <- fare_amount_log ~ pickup_longitude + pickup_latitude + 
  dropoff_longitude + dropoff_latitude + trip_distance + year + month + day + hour + 
  passenger_count + pickup_cluster +
  feat01 + feat02 + feat03 + feat04 + feat05 + feat06 + feat07 + feat08 + feat09 + feat10 
```


## Feature Selection 

```{r, cache=TRUE}
# Fit a Random Forest model
rf_model <- randomForest(model.formula, data = data.train)

# Extract feature importance
importance(rf_model)
```

```{r}
saveRDS(rf_model, here("rf_model.rds"))
```

```{r}
rf_model <-readRDS("../rf_model.rds")
```

```{r}
varImpPlot(rf_model, main = "Variable Importance Plot from Random Forest")
print(rf_model)
plot(rf_model)
```



### XGBoost with ANOVA Racing tuning

```{r}
# Define model formula 
model.formula <- fare_amount_log ~ pickup_longitude + pickup_latitude + 
  dropoff_longitude + dropoff_latitude + trip_distance + month + day + hour + 
  passenger_big_group + pickup_cluster +
  feat01 + feat02 + feat03 + feat04 + feat05 + feat06 + feat07 + feat08 + feat09 + feat10 
```



```{r}
# Preprocessing step 

preprocessing_recipe <- 
  recipes::recipe(model.formula, data = data.train) %>% 
  recipes::step_string2factor(all_nominal()) %>% 
  recipes::step_other(all_nominal(), threshold = 0.01) %>% 
  #step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% 
  recipes::step_nzv(all_predictors()) %>% 
  prep()
```

```{r}
# Cross-validation Step 

cv_folds <- 
  recipes::bake(
    preprocessing_recipe, 
    new_data = data.train
  ) %>% 
  rsample::vfold_cv(v=5)
```


```{r}
# Model specification 

model_xgboost <- 
  parsnip::boost_tree(
    mode = "regression", 
    trees = 1000, 
    min_n = tune(), 
    tree_depth = tune(), 
    learn_rate = 0.05, 
    loss_reduction = tune(), 
    mtry = tune()
  ) %>% 
  set_engine("xgboost")
```


```{r}
params_xgboost <- dials::parameters(
    min_n(), 
    tree_depth(), 
    loss_reduction(), 
    finalize(mtry(), data.train)
)
```

```{r}
# Set up grid to tune on 

grid_xgboost <- dials::grid_latin_hypercube(
  params_xgboost, 
  size = 5
)

grid_xgboost
```


```{r}
workflow_xgboost <- workflows::workflow() %>% 
  add_model(model_xgboost) %>% 
  add_formula(model.formula)
  
```


```{r, cache=TRUE}
doParallel::registerDoParallel()

set.seed(777)

xgboost_race <- tune_race_anova(
  workflow_xgboost, 
  cv_folds, 
  grid = grid_xgboost, 
  metrics = yardstick::metric_set(rmse, rsq, mae, mape, mpe), 
  control = control_race(verbose_elim = TRUE)
)
```


```{r, cache=TRUE}
show_best(xgboost_race, metric =  'mape')
```
```{r}
plot_race(xgboost_race)
```


```{r}

xgboost_last <- workflow_xgboost %>% 
  finalize_workflow(select_best(xgboost_race, "rmse")) %>% 
  last_fit(train_test_split)

xgboost_last
```
```{r}
collect_metrics(xgboost_last)
```



### Random Forest with ANOVA Racing 

```{r}
model_ranger <- parsnip::rand_forest(
  mtry = tune(), min_n = tune(), trees = 1000
) %>% 
  set_mode("regression") %>% 
  set_engine("ranger")
```

```{r}
workflow_ranger <- workflow() %>% 
  add_model(model_ranger) %>% 
  add_formula(model.formula)
```


```{r}
doParallel::registerDoParallel()

set.seed(564553)
ranger_race <- tune_race_anova(workflow_ranger, 
  cv_folds, 
  grid = 15, 
  metrics = yardstick::metric_set(rmse, rsq, mae, mape, mpe), 
  control = control_race(verbose_elim = TRUE))

```

```{r}
show_best(ranger_race)
```


```{r, cache=TRUE}
ranger_last <- workflow_ranger %>% 
  finalize_workflow(select_best(ranger_race, "rmse")) %>% 
  last_fit(split = train_test_split)

ranger_last
```

```{r}
bind_rows(list(rf = collect_metrics(ranger_last), xgb =  collect_metrics(xgboost_last)), .id = "id")
```

### Decision Tree 


```{r}
model_decision_tree <- 
    decision_tree(
      tree_depth = tune(), 
      cost_complexity = tune(), 
      min_n = tune()) %>% 
 
    set_mode("regression") %>% 
    set_engine("rpart")
```


```{r}
tree_grid <- grid_regular(cost_complexity(), 
                          tree_depth(), 
                          min_n(), levels = 3)
tree_grid
```

```{r}
doParallel::registerDoParallel()

set.seed(145252)

decis_tree <- tune_grid(model_decision_tree, 
                        model.formula, 
                        resamples = cv_folds, 
                        grid = tree_grid, 
                        metrics = yardstick::metric_set(rmse, rsq, mae, mape, mpe))

decis_tree
```


```{r}
show_best(decis_tree) 
```

```{r}
final_tree <- finalize_model(model_decision_tree, select_best(decis_tree, 'rmse'))
```


```{r, cache=TRUE}
tree_last <- last_fit(final_tree, model.formula, train_test_split)
```



```{r}
bind_rows(list(rf = collect_metrics(ranger_last), xgb =  collect_metrics(xgboost_last), dt = collect_metrics(tree_last)), .id = "id")
```
